#!/usr/bin/env python3
"""
Facebook Comments Scraper
==========================
Ø³ÙƒØ±Ø¨Øª Ù„Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª Ù…Ù† Ø¨ÙˆØ³Øª ÙÙŠØ³Ø¨ÙˆÙƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GraphQL API
"""

import json
import re
import requests
import sys
import time
import urllib.parse
import base64
from datetime import datetime


class FacebookCommentsScraper:
    """ÙØ¦Ø© Ø¬Ù„Ø¨ ÙƒÙˆÙ…Ù†ØªØ§Øª ÙÙŠØ³Ø¨ÙˆÙƒ"""
    
    def __init__(self, cookies_file="cookies.json"):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙØ¦Ø© Ù…Ø¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ²"""
        self.session = requests.Session()
        self.cookies_file = cookies_file
        self.fb_dtsg = None
        self.lsd = None
        self.jazoest = "25515"  # Ù†ÙØ³ Ø§Ù„Ù‚ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…Ø¨Ø³Ø·
        self.user_id = None
        
    def load_cookies(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù…Ù† Ù…Ù„Ù JSON"""
        try:
            print("ğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ²...")
            with open(self.cookies_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©
            if isinstance(data, list):
                for cookie in data:
                    if cookie.get('domain') == '.facebook.com':
                        self.session.cookies.set(
                            cookie['name'], 
                            cookie['value'], 
                            domain=cookie['domain']
                        )
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† c_user
                        if cookie['name'] == 'c_user':
                            self.user_id = cookie['value']
                            
                print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ø¨Ù†Ø¬Ø§Ø­")
                if self.user_id:
                    print(f"ğŸ‘¤ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {self.user_id}")
                return True
            else:
                print("âŒ ÙÙˆØ±Ù…Ø§Øª Ù…Ù„Ù Ø§Ù„ÙƒÙˆÙƒÙŠØ² ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…")
                return False
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ²: {e}")
            return False

    def extract_tokens(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ù…Ù† ÙÙŠØ³Ø¨ÙˆÙƒ - Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© extract_tokens.py"""
        try:
            print("ğŸ” Ø¬Ø§Ø±ÙŠ Ø²ÙŠØ§Ø±Ø© ÙÙŠØ³Ø¨ÙˆÙƒ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø²...")
            
            # Ø²ÙŠØ§Ø±Ø© ØµÙØ­Ø© ÙÙŠØ³Ø¨ÙˆÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            response = self.session.get('https://www.facebook.com/', timeout=30)
            
            if response.status_code != 200:
                print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ ØµÙØ­Ø© ÙÙŠØ³Ø¨ÙˆÙƒ: {response.status_code}")
                return False
            
            page_content = response.text
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ fb_dtsg - Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù…Ù† extract_tokens.py
            fb_dtsg = None
            dtsg_patterns = [
                r'"DTSGInitialData",\[\],\{"token":"([^"]+)"',
                r'"dtsg":\{"token":"([^"]+)"',
                r'fb_dtsg":"([^"]+)"',
                r'DTSGInitialData.*?"token":"([^"]+)"'
            ]
            
            for pattern in dtsg_patterns:
                dtsg_match = re.search(pattern, page_content)
                if dtsg_match:
                    fb_dtsg = dtsg_match.group(1)
                    break
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ lsd - Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù…Ù† extract_tokens.py
            lsd = None
            lsd_patterns = [
                r'"LSD",\[\],\{"token":"([^"]+)"',
                r'"token":"([^"]{20,})"'
            ]
            
            for pattern in lsd_patterns:
                lsd_match = re.search(pattern, page_content)
                if lsd_match:
                    lsd = lsd_match.group(1)
                    break
            
            # jazoest Ù‡Ùˆ Ù‚ÙŠÙ…Ø© Ø«Ø§Ø¨ØªØ© - Ù†ÙØ³ extract_tokens.py
            jazoest = "25515"
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙˆÙƒÙ†Ø² ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³
            self.fb_dtsg = fb_dtsg
            self.lsd = lsd
            self.jazoest = jazoest
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            if self.fb_dtsg:
                print(f"âœ… fb_dtsg: {self.fb_dtsg}")
            else:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ fb_dtsg")
                
            if self.lsd:
                print(f"âœ… lsd: {self.lsd}")
            else:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ lsd")
                
            print(f"âœ… jazoest: {self.jazoest}")
            
            # Ø¯Ø¹Ù‡ ÙŠÙƒÙ…Ù„ Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ÙŠØ¬Ø¯ lsd - ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ ÙÙŠ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù…Ø¨Ø³Ø·
            if self.fb_dtsg:
                print("âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø² Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© - Ø³ÙŠØªÙ… Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©")
                return True
            else:
                print("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ fb_dtsg - Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©")
                return False
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø²: {e}")
            return False

    def analyze_post_url(self, post_url):
        """ØªØ­Ù„ÙŠÙ„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ø¨ÙˆØ³Øª"""
        print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³Øª:")
        print(f"ğŸ“ {post_url}")
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø·
        post_url = post_url.strip()
        
        # ØªØ­Ù„ÙŠÙ„ URL
        parsed_url = urllib.parse.urlparse(post_url)
        query_params = urllib.parse.parse_qs(parsed_url.query)
        
        post_info = {
            "post_id": None,
            "user_id": None,
            "pfbid": None,
            "feedback_id": None
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ø¨ÙˆØ³Øª
        if "permalink.php" in parsed_url.path:
            # Ù…Ù† Ù†ÙˆØ¹ permalink
            if 'story_fbid' in query_params:
                story_fbid = query_params['story_fbid'][0]
                post_info["post_id"] = story_fbid
                
                if story_fbid.startswith('pfbid'):
                    post_info["pfbid"] = story_fbid
                    print(f"ğŸ” Ù…Ø¹Ø±Ù Ù…Ø´ÙØ± (pfbid): {story_fbid}")
                else:
                    print(f"ğŸ”¢ Ù…Ø¹Ø±Ù Ø±Ù‚Ù…ÙŠ: {story_fbid}")
            
            if 'id' in query_params:
                post_info["user_id"] = query_params['id'][0]
                print(f"ğŸ‘¤ Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…/Ø§Ù„ØµÙØ­Ø©: {post_info['user_id']}")
        
        elif "/posts/" in parsed_url.path:
            # Ù…Ù† Ù†ÙˆØ¹ direct post
            post_match = re.search(r'/posts/([^/?]+)', parsed_url.path)
            if post_match:
                post_id = post_match.group(1)
                post_info["post_id"] = post_id
                
                if post_id.startswith('pfbid'):
                    post_info["pfbid"] = post_id
                    print(f"ğŸ” Ù…Ø¹Ø±Ù Ù…Ø´ÙØ± (pfbid): {post_id}")
                else:
                    print(f"ğŸ”¢ Ù…Ø¹Ø±Ù Ø±Ù‚Ù…ÙŠ: {post_id}")
        
        # ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰ feedback_id Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†
        if post_info["pfbid"]:
            feedback_id = self.convert_pfbid_to_feedback_id(post_info["pfbid"])
            if feedback_id:
                post_info["feedback_id"] = feedback_id
                print(f"ğŸ¯ Feedback ID: {feedback_id}")
        
        return post_info

    def convert_pfbid_to_feedback_id(self, pfbid):
        """ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰ feedback_id Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Base64"""
        try:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© pfbid
            encoded_part = pfbid[5:]
            
            # Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ© Ù„Ù„ØªØ­ÙˆÙŠÙ„
            methods = [
                # Ø·Ø±ÙŠÙ‚Ø© 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… pfbid Ù…Ø¨Ø§Ø´Ø±Ø© ÙƒÙ€ feedback_id
                lambda x: base64.b64encode(f"feedback:{pfbid}".encode()).decode(),
                
                # Ø·Ø±ÙŠÙ‚Ø© 2: Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙƒ pfbid Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙÙŠØ±Ù‡
                lambda x: self._try_decode_pfbid(x),
                
                # Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… hash Ù…Ù† pfbid
                lambda x: self._hash_pfbid_to_feedback(pfbid)
            ]
            
            for i, method in enumerate(methods):
                try:
                    result = method(encoded_part)
                    if result:
                        print(f"âœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© {i+1}")
                        return result
                except Exception as e:
                    print(f"âš ï¸ ÙØ´Ù„Øª Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© {i+1}: {e}")
                    continue
            
            # Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ØŒ Ø§Ø³ØªØ®Ø¯Ù… pfbid ÙƒÙ…Ø§ Ù‡Ùˆ
            print("âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… pfbid ÙƒÙ…Ø§ Ù‡Ùˆ")
            return pfbid
            
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­ÙˆÙŠÙ„ pfbid: {e}")
            return pfbid

    def _try_decode_pfbid(self, encoded_part):
        """Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙƒ pfbid"""
        try:
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø­Ø±Ù URL-safe
            modified = encoded_part.replace('-', '+').replace('_', '/')
            
            # Ø¥Ø¶Ø§ÙØ© padding
            padding = len(modified) % 4
            if padding:
                modified += '=' * (4 - padding)
            
            decoded = base64.b64decode(modified)
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ feedback format
            feedback_data = f"feedback:{decoded.hex()[:16]}"
            return base64.b64encode(feedback_data.encode()).decode()
            
        except:
            return None

    def _hash_pfbid_to_feedback(self, pfbid):
        """ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰ feedback_id Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… hash"""
        try:
            import hashlib
            
            # Ø¥Ù†Ø´Ø§Ø¡ hash Ù…Ù† pfbid
            hash_obj = hashlib.sha256(pfbid.encode())
            hash_hex = hash_obj.hexdigest()[:20]
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ feedback format
            feedback_data = f"feedback:{hash_hex}"
            return base64.b64encode(feedback_data.encode()).decode()
            
        except:
            return None

    def get_comments(self, feedback_id, cursor=None, max_pages=None):
        """Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª Ù…Ù† Ø§Ù„Ø¨ÙˆØ³Øª"""
        try:
            print(f"\nğŸ’¬ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª...")
            if cursor:
                print(f"ğŸ“„ Ù…Ù† Ø§Ù„ØµÙØ­Ø©: {cursor[:20]}...")
            
            # ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰ format ØµØ­ÙŠØ­ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            graphql_id = feedback_id
            if feedback_id.startswith('pfbid'):
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰ feedback format
                try:
                    feedback_data = f"feedback:{feedback_id}"
                    graphql_id = base64.b64encode(feedback_data.encode()).decode()
                    print(f"ğŸ”„ ØªÙ… ØªØ­ÙˆÙŠÙ„ pfbid Ø¥Ù„Ù‰: {graphql_id[:50]}...")
                except:
                    # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ØŒ Ø§Ø³ØªØ®Ø¯Ù… pfbid ÙƒÙ…Ø§ Ù‡Ùˆ
                    graphql_id = feedback_id
                    print(f"âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… pfbid Ù…Ø¨Ø§Ø´Ø±Ø©: {graphql_id}")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù„Ù„Ø·Ù„Ø¨
            variables = {
                "commentsAfterCount": -1,
                "commentsAfterCursor": cursor,
                "commentsBeforeCount": None,
                "commentsBeforeCursor": None,
                "commentsIntentToken": "RANKED_FILTERED_INTENT_V1",
                "feedLocation": "POST_PERMALINK_DIALOG",
                "focusCommentID": None,
                "scale": 2,
                "useDefaultActor": False,
                "id": graphql_id,
                "__relay_internal__pv__IsWorkUserrelayprovider": False
            }
            
            # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù„Ø¨
            data = {
                "av": self.user_id,
                "__aaid": "0",
                "__user": self.user_id,
                "__a": "1",
                "__req": "3n",
                "__hs": "20325.HYP:comet_pkg.2.1...0",
                "dpr": "1",
                "__ccg": "EXCELLENT",
                "__rev": "1026303884",
                "__s": "43pu0c:crpsn8:ehpbtp",
                "__hsi": "7542672544110411392",
                "__dyn": "7xeUjGU5a5Q1ryaxG4Vp41twWwIxu13wFwhUKbgS3q2ibwNw9G2Saw8i2S1DwUx60GE5O0BU2_CxS320qa321Rwwwqo462mcwfG12wOx62G5Usw9m1YwBgK7o6C0Mo4G17yovwRwlE-U2exi4UaEW2G1jwUBwJK14xm3y11xfxmu3W3jU8o4Wm7-2K0-obUG2-azqwaW223908O3216xi4UK2K2WEjxK2B08-269wkopg6C13xecwBwWwjHDzUiBG2OUqwjVqwLwHwa211wo83KwHwOyUqxG",
                "__csr": "g4X1gAxq2AeigL3YbOiOiNQ_dNcAhv9sl5OEIKOhadqRYLjW4RH69isRqZGriO-zTC4WAqOqHKhrXJkACJnBHh9a_syt6Fyk8Ju-BOaJOJGpelbBWRy9lQaECt9pP9ainJeumqqF9Uyb8DByrK4aF34V2p9bz95XzaBJyAVAbLK5GV97l95zbLDWQO12VGGHKeJdHx6uiiEGmmlpEG8G48-qnx54G8WQHJ-QK4F-ehaKCdxd1N3qxFeF8Caz4GJ9qKcBUHK8VUy49pUpDzFUSKayUCWxbhpAbAzFrBz8-FFpXCCABG6pEW8JyppUWlFeewDzoCEGVoK3DyEmwFx91WVE9bKEKbAwg48yUgDx92oyazayUC7oOu9xObiwBwkEqwBzUc8c8Op38WEO48gHyoCfGezobEa98y484mcG2S14wGy4ewCxG9DwmGw_AwmoG3bGi7Q0YEfoa84a0GoW0AFU9k9xuFolK0BGy6Hg7qWwGweK1bwzCwVzHxi3C5o2PwuEWu8ADwg86q6o7a1OwdyENajw0Mtw1-R0dmu0bfxS8wuo0Aq7o1RF81NU1gE4p01mubxS1_w0ajK1Cw0Ysyu0gW1OBwaYM2Vwee3y0s1wcu9waZw7oxC0AA0kIw8E0Xi1ko2ZgG1gz8y3i582Fwe2780Gh03BEDa0f-w1lW09DysElw3oU4p015W0hmE09TEcy02OEOt04tgozF2wEw1OC1Fw4Rw7Lw5GxO3e1ew2_80pnwuE1uU13tw7gyE27F05Ow2Kyw98E",
                "__comet_req": "15",
                "fb_dtsg": self.fb_dtsg,
                "jazoest": self.jazoest,
                "lsd": self.lsd,
                "__spin_r": "1026303884",
                "__spin_b": "trunk",
                "__spin_t": str(int(time.time())),
                "fb_api_caller_class": "RelayModern",
                "server_timestamps": "true",
                "fb_api_req_friendly_name": "CommentsListComponentsPaginationQuery",
                "variables": json.dumps(variables),
                "doc_id": "24170828295923210"
            }
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ headers Ù„Ù„Ø·Ù„Ø¨
            headers = {
                'Accept': '*/*',
                'Accept-Language': 'en-US,en;q=0.9,ar;q=0.8',
                'Content-Type': 'application/x-www-form-urlencoded',
                'Origin': 'https://www.facebook.com',
                'Referer': 'https://www.facebook.com/',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
            }
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨
            response = self.session.post(
                'https://www.facebook.com/api/graphql/',
                data=data,
                headers=headers,
                timeout=30
            )
            
            if response.status_code != 200:
                print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø·Ù„Ø¨: {response.status_code}")
                return None, None
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            try:
                response_data = response.json()
                return self.parse_comments_response(response_data)
            except json.JSONDecodeError:
                print("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ JSON response")
                return None, None
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª: {e}")
            return None, None

    def parse_comments_response(self, response_data):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª"""
        try:
            comments = []
            next_cursor = None
            
            # Ø§Ù„ØªÙ†Ù‚Ù„ ÙÙŠ Ø¨Ù†ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¬Ø±Ø¨Ø©
            data = response_data.get('data', {})
            node = data.get('node', {})
            
            if not node:
                print("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ node ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                return [], None
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª
            comment_rendering = node.get('comment_rendering_instance_for_feed_location', {})
            comments_data = comment_rendering.get('comments', {})
            
            if not comments_data:
                print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙˆÙ…Ù†ØªØ§Øª")
                return [], None
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ pagination info
            page_info = comments_data.get('page_info', {})
            if page_info:
                has_next_page = page_info.get('has_next_page', False)
                if has_next_page:
                    next_cursor = page_info.get('end_cursor')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª
            edges = comments_data.get('edges', [])
            
            for edge in edges:
                comment_node = edge.get('node', {})
                comment_data = self.extract_comment_data(comment_node)
                if comment_data:
                    comments.append(comment_data)
            
            print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(comments)} ÙƒÙˆÙ…Ù†Øª")
            if next_cursor:
                print(f"ğŸ“„ ÙŠÙˆØ¬Ø¯ ØµÙØ­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©")
            
            return comments, next_cursor
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {e}")
            return [], None

    def extract_comment_data(self, comment_node):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒÙˆÙ…Ù†Øª"""
        try:
            comment = {}
            
            # Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            comment['id'] = comment_node.get('id', '')
            comment['created_time'] = comment_node.get('created_time', 0)
            
            # Ø§Ù„Ù†Øµ
            body = comment_node.get('body', {})
            comment['text'] = body.get('text', '') if body else ''
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¤Ù„Ù
            author = comment_node.get('author', {})
            if author:
                comment['author_name'] = author.get('name', 'Unknown')
                comment['author_id'] = author.get('id', '')
                comment['author_url'] = author.get('url', '')
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ù„Ø±Ø¯ÙˆØ¯
            feedback = comment_node.get('feedback', {})
            if feedback:
                # Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª
                reaction_count = feedback.get('reaction_count', {})
                comment['likes_count'] = reaction_count.get('count', 0) if reaction_count else 0
                
                # Ø§Ù„Ø±Ø¯ÙˆØ¯
                replies_fields = feedback.get('replies_fields', {})
                comment['replies_count'] = replies_fields.get('total_count', 0) if replies_fields else 0
            else:
                comment['likes_count'] = 0
                comment['replies_count'] = 0
            
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙˆÙ‚Øª
            if comment['created_time']:
                try:
                    dt = datetime.fromtimestamp(comment['created_time'])
                    comment['created_time_formatted'] = dt.strftime('%Y-%m-%d %H:%M:%S')
                except:
                    comment['created_time_formatted'] = 'Unknown'
            else:
                comment['created_time_formatted'] = 'Unknown'
            
            return comment
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙˆÙ…Ù†Øª: {e}")
            return None

    def scrape_all_comments(self, post_url, delay=10, max_pages=None):
        """Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª Ù…Ù† Ø§Ù„Ø¨ÙˆØ³Øª"""
        print("=" * 80)
        print("ğŸ” Facebook Comments Scraper")
        print("=" * 80)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        if not self.load_cookies():
            return None
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙƒÙ†Ø²
        if not self.extract_tokens():
            return None
        
        # ØªØ­Ù„ÙŠÙ„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³Øª
        post_info = self.analyze_post_url(post_url)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ø±Ù Ø§Ù„Ø¨ÙˆØ³Øª Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ GraphQL
        post_identifier = None
        if post_info.get('feedback_id'):
            post_identifier = post_info['feedback_id']
            print(f"ğŸ“‹ Ø§Ø³ØªØ®Ø¯Ø§Ù… feedback_id: {post_identifier[:30]}...")
        elif post_info.get('pfbid'):
            post_identifier = post_info['pfbid']
            print(f"ğŸ“‹ Ø§Ø³ØªØ®Ø¯Ø§Ù… pfbid Ù…Ø¨Ø§Ø´Ø±Ø©: {post_identifier}")
        elif post_info.get('post_id'):
            post_identifier = post_info['post_id']
            print(f"ğŸ“‹ Ø§Ø³ØªØ®Ø¯Ø§Ù… post_id: {post_identifier}")
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø±Ù Ø§Ù„Ø¨ÙˆØ³Øª")
            return None
        
        # Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª
        all_comments = []
        cursor = None
        page_count = 0
        
        while True:
            page_count += 1
            print(f"\nğŸ“„ ØµÙØ­Ø© {page_count}:")
            
            comments, next_cursor = self.get_comments(post_identifier, cursor, max_pages)
            
            if not comments:
                print("âŒ Ù„Ù… ÙŠØªÙ… Ø¬Ù„Ø¨ Ø£ÙŠ ÙƒÙˆÙ…Ù†ØªØ§Øª")
                break
            
            all_comments.extend(comments)
            
            if not next_cursor or (max_pages and page_count >= max_pages):
                break
            
            cursor = next_cursor
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ
            print(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {delay} Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„ØªØ§Ù„ÙŠ...")
            time.sleep(delay)
        
        return {
            'post_url': post_url,
            'post_info': post_info,
            'total_comments': len(all_comments),
            'pages_scraped': page_count,
            'comments': all_comments,
            'scraped_at': datetime.now().isoformat()
        }

    def save_results(self, results, filename=None):
        """Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù JSON"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"facebook_comments_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {e}")
            return None

    def display_summary(self, results):
        """Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        if not results:
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        print("=" * 80)
        
        print(f"ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³Øª: {results['post_url']}")
        print(f"ğŸ’¬ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª: {results['total_comments']}")
        print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {results['pages_scraped']}")
        print(f"ğŸ• ÙˆÙ‚Øª Ø§Ù„Ø¬Ù„Ø¨: {results['scraped_at']}")
        
        if results['comments']:
            print(f"\nğŸ’¬ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª:")
            print("-" * 40)
            
            for i, comment in enumerate(results['comments'][:5]):
                print(f"\n{i+1}. {comment.get('author_name', 'Unknown')}")
                print(f"   ğŸ“… {comment.get('created_time_formatted', 'Unknown')}")
                print(f"   â¤ï¸ {comment.get('likes_count', 0)} Ø¥Ø¹Ø¬Ø§Ø¨")
                text = comment.get('text', '')[:100]
                if len(comment.get('text', '')) > 100:
                    text += "..."
                print(f"   ğŸ’­ {text}")
        
        print("=" * 80)


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    # Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_url = "https://www.facebook.com/bantalrb.heba/posts/pfbid029Ra2EaMcbGn5miZxq5DHx2cS3NYiYGav7TBMzFWQ8JQ8fvs3T1BGpVKnR4sN44LVl"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙƒØ´Ø·Ø© Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª
    scraper = FacebookCommentsScraper()
    
    # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª
    results = scraper.scrape_all_comments(
        post_url=test_url,
        delay=10,  # 10 Ø«ÙˆØ§Ù†Ù Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
        max_pages=None  # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª
    )
    
    if results:
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ
        scraper.display_summary(results)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        scraper.save_results(results)
    else:
        print("âŒ ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª")


if __name__ == "__main__":
    main()
